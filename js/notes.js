


var mData = [
    { "td1": "Model Size", "td2": "The overall size of the model, often measured in terms of parameters (e.g., 7B for 7 billion parameters)." },
    { "td1": "Training Tokens", "td2": "The number of tokens in the training dataset, indicating the scale of the data used for model training (e.g., 1.4T for 1.4 trillion tokens)." },
    { "td1": "Context Length", "td2": "The maximum number of tokens considered as context during inference or generation (e.g., 2048 tokens)." },
    { "td1": "Architecture", "td2": "The underlying structure and design of the model, such as transformer-based architectures like GPT (Generative Pre-trained Transformer) or BERT (Bidirectional Encoder Representations from Transformers)." },
    { "td1": "Tokenization", "td2": "The method used to break down text into tokens, which can be words, subwords, or characters, depending on the chosen strategy." },
    { "td1": "Training Data", "td2": "Information about the source and diversity of the training dataset, providing insights into the model's exposure to different language patterns and contexts." },
];